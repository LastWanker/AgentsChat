# weight 生成合理性分析（SystemAnalysis/weight生成合理性分析.md）

> 主题：如何避免幻觉，使 weight 更合理？是否需要事后评估，而不是让 LLM 一边说话一边“脑补”？

---

## 1. 先把问题说清楚：weight 是什么？
在系统里，引用（Reference）带有三维权重：
- **stance**：立场（-1~1，反对到支持）
- **inspiration**：启发程度（0~1）
- **dependency**：依赖程度（0~1）

它的本意是：
> 不仅告诉你“引用了谁”，还告诉你“我跟他关系有多深”。

这是一个很高级的设计，但问题也很大：
**LLM 很容易在“没有证据”的情况下瞎给分。**

---

## 2. 现状分析：LLM 在哪里“容易幻觉”？

### 2.1 LLM 在生成内容的同时给 weight
- 现在的 prompt 要求 LLM 在 finalize 阶段输出 weight。
- 这意味着：**模型在“说话时同时判断关系强度”**。

问题：
- 说话时本来就容易幻想“自己理解了多少”，再加上权重打分，等于“双倍主观”。

### 2.2 没有“地面真相”来校正
- 系统目前没有“真实评分”或“结果反馈”。
- 一旦模型乱打分，就很难纠正。

总结一句话：
> 现在的权重，更多是“心情指数”，不是“证据指数”。

---

## 3. 如何避免幻觉？（三种方向）

### 3.1 事后评估（强烈建议）
把“权重生成”拆成两步：
1. **LLM 发言**：只负责给出引用列表（event_id）。
2. **事后评估器**：单独计算 weight。

好处：
- 避免“边说边拍脑袋”。
- 可以引入更客观的打分机制。

### 3.2 多模型交叉校验
- 让多个模型分别给 weight，然后求平均/投票。
- 类似“评审委员会”，降低单模型偏差。

### 3.3 引入可解释的规则打分
- 对 weight 设定规则化标准：
  - 如果引用事件出现在 retrieval_plan 中，则 dependency ↑
  - 如果引用事件是 request，则 stance 不能为 0
- 让权重有可追溯逻辑，而不是纯“主观感觉”。

---

## 4. 是否需要事后评估？
**结论：需要，而且建议作为强制流程。**

原因：
- weight 是“系统解释层”的关键数据，错误会累积放大。
- 事后评估可以独立优化，而不影响 Agent 主流程。

一句话总结：
> 让 LLM 负责说话，把“评价”交给另一个模块，这是更合理的分工。

---

## 5. 推荐方案（可落地版本）

### 方案 A：两段式权重生成
- Step1：LLM 输出 final 内容 + references（无 weight）。
- Step2：WeightEvaluator 根据以下信息计算权重：
  - 引用事件内容
  - trigger_event
  - 当前 Agent 的角色
  - retrieval_plan / candidate_events

### 方案 B：混合评分
- 由规则先给一个基础权重
- LLM 只做“微调”

公式感示意：
```
final_weight = 0.7 * rule_weight + 0.3 * llm_weight
```

---

## 6. 结论（通俗版）
- 当前 weight 更像“主观体感”，不是“客观证据”。
- 建议把 weight 从 LLM 发言里拆出来，交给后置评估模块。
- 这样既能减少幻觉，又能让系统更可信。

一句话收尾：
> 让 LLM 先把“话说清楚”，别急着让它给“感情打分”。评估交给后厨，菜才稳定。

