1. 核心流程：到底是 “query 需要 intention” 还是 “intention 需要 query”？
你说得更像人类的认知过程：先有意图，再翻旧账。
我站你这边——但我会把它拆成两段，避免你付出巨大 token 代价：
我建议的两段式（这是最稳的） IntentionDraft（不含 references）
输入：最近 10~50 条事件（或更少）
输出：kind + 要说什么/要做什么 + retrieval_plan（我要查什么）
Query/Resolver（按 retrieval_plan 查）
输出：候选 event_id 列表（以及必要的片段） FinalIntention（含 references + 自然语言）
输入：draft + 查询到的证据
输出：最终 intention（references 填满）+ 最终 speak 文本 这就把你说的“看最近 → 决定要翻哪些旧账 → 翻旧账 → 引用+发言”落成了工程流程。

2.你担心的“每次都往前翻会很花钱”怎么解决？
靠三件套，不玄学： 滚动摘要（scope 级别）：每个 scope（public / group:x）
维护一个短摘要（几百 token），每次新事件进来，摘要增量更新。
LLM 做 draft 时主要吃摘要，必要时再取最近 N 条原文。
事件分层：原始事件是事实层；摘要是语义层；
更高一层可以是“争议点列表 / TODO 列表 / 决策状态”。你这系统本来就适合做层次化记忆。
缓存：同一个 scope 最近 N 条其实高度重叠；你可以缓存“最近窗口→草稿意图”的中间表示（至少在同一个 tick 内）。“

针对intention的policy（告诉llm如何分两步产生这些用来生成一个良好intention的schema）
有必要和针对event的intent_proposal.yaml也就是告诉llm你需要遵守的event类型的，这二者有必要放在一起吗？
结论：强烈建议拆开。
理由很简单：它们管的不是同一件事。
事件类型 policy（intent_proposal.yaml）：是“世界允许你做什么”。它像法典/接口文档，稳定、慢变、面向系统。
意向生成流程 policy（给 LLM 的两段式）：是“你应该怎么思考、怎么产出”。它像操作手册/驾驶指南，迭代会更快，而且很可能随模型版本/提示词策略频繁调整。

下一步计划（现在该干什么）

一个能直接推进代码结构的路线，按重要性排序：
1) 把“两段式意向生成”落成明确的 Schema（先别管 LLM，先把形状钉死）
你需要两个 schema（可以是 Python TypedDict / pydantic / json schema，随你习惯）：
A. IntentionDraft（不含 references）
kind: 你现有意向类型（speak/submit/…）
target_scope / visibility（如果你要）
message_plan: 一句话说明“我打算说什么/做什么”（不是最终自然语言）
retrieval_plan: 列表，描述要查什么（关键词、事件类型过滤、时间窗、scope、需要 thread/引用链深度等）
B. FinalIntention（含 references + 最终文本）
kind
content（最终 speak 文本 / submit payload）
references: 你三维 weight 的标准结构（event_id + weight{stance/inspiration/dependency}）
completed（如果还需要）
可选：candidate_references 留给 debug
这一步做完，你的系统就有了“生产线的模具”。

2) 做一个 ReferenceResolver（不靠 LLM 也能跑）
两段式里最关键的一段不是 query 本身，而是resolver 把检索结果变成 references。
Resolver 负责：
按 retrieval_plan 调用 query 拿候选事件列表
选择 topK（先简单：最近优先 + 类型匹配 + 关键词命中数）
生成 references（默认三维权重怎么填：可以先全 0，再由 LLM 在 FinalIntention 阶段填；stance/inspiration 留给 LLM）
注意：你说“reference 必须也只能是 query 之后才能形成”——我同意。那就让 resolver 成为强制门槛：FinalIntention 不经过 resolver 不准落库。

3) Query 扩展到“够 resolver 用”的最低集合
你现在 query 只会 last_n / get_by_id 的话，resolver 会很瘸。
最小可用集合我建议三招（先朴素字符串 contains 就行）：
recent(scope, n)
search(scope, keywords, limit, event_types=None, after_id=None/after_time=None)
thread_up(event_id, depth)（沿 references 往上追溯，拿上下文链）
你别急着做向量库、embedding，那是后话。先用笨办法把链路跑通，再替换检索策略。

4) 写“给 LLM 的 DraftPolicy”文件（拆开的那个）
文件名我建议直白点，比如：
policies/intent_production.yaml（或 intent_generation.yaml）
内容包含两块：
Draft 阶段的输出规范：要求给出 retrieval_plan，禁止直接写 references
Finalize 阶段的输出规范：必须基于 resolver 返回的候选事件（event_id 列表+片段）来选引用并填三维 weight；禁止编造不存在的 event_id
这份文件的角色是：提示词规约 + 自检清单，不是系统硬约束。硬约束应该在 interpreter/resolver 里。

5) 把 intent_proposal.yaml 保持“法典化”
intent_proposal.yaml 就继续只管：
有哪些 event_type / action
每个 action 需要的字段
references 是否允许、允许多少条
weight 的取值范围（即便你暂时不严格执行，也应该写在“法典”里当未来约束）
一句话：它定义世界的语法，不定义写作方法。